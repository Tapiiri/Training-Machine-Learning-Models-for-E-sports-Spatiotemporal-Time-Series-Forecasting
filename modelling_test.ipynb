{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "import sqlite3\n",
    "import operator as op\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils.get_or_create_combined_database import get_or_create_combined_database\n",
    "from utils.create_compound_key_and_index import create_compound_key_and_index\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from constants import DB_columns, GAME_AREA_WIDTH\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(verbose=True, override=True)\n",
    "\n",
    "RECREATE_CLEANED_DATA = False\n",
    "\n",
    "zoom_range = ((75, 14350), (75, 14350))\n",
    "normalized_zoom_range = ((0, 1), (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_folder = os.getenv(\"DATABASE_FOLDER\")\n",
    "\n",
    "database_file = get_or_create_combined_database(database_folder)\n",
    "\n",
    "conn = sqlite3.connect(database_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "tlol_db_table_name = \"champs\"\n",
    "\n",
    "page_size = 5\n",
    "\n",
    "rows = cursor.execute(\n",
    "    f\"SELECT * FROM {tlol_db_table_name} LIMIT {page_size}\").fetchall()\n",
    "conn.close()\n",
    "print(f\"Loaded {len(rows)} rows from the database as a test\")\n",
    "\n",
    "rows[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_table_name = f\"{tlol_db_table_name}_cleaned\"\n",
    "\n",
    "\n",
    "# Create a new table for cleaned up data\n",
    "if RECREATE_CLEANED_DATA:\n",
    "    conn = sqlite3.connect(database_file)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Drop previous table\n",
    "    cursor.execute(f\"DROP TABLE IF EXISTS {cleaned_table_name}\")\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    cursor.execute(\n",
    "        f\"CREATE TABLE {cleaned_table_name} AS SELECT * FROM {tlol_db_table_name} WHERE 1=0\")\n",
    "\n",
    "    # Add normalized columns\n",
    "\n",
    "    cursor.execute(\n",
    "        f\"ALTER TABLE {cleaned_table_name} ADD COLUMN {DB_columns.NORMALIZED_POS_X.value} FLOAT GENERATED ALWAYS AS ({DB_columns.POS_X.value} / {GAME_AREA_WIDTH}) STORED\")\n",
    "    cursor.execute(\n",
    "        f\"ALTER TABLE {cleaned_table_name} ADD COLUMN {DB_columns.NORMALIZED_POS_Z.value} FLOAT GENERATED ALWAYS AS ({DB_columns.POS_Z.value} / {GAME_AREA_WIDTH}) STORED\")\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data to the new table from original table according to a filter\n",
    "\n",
    "# Conditions:\n",
    "# Only rows with a name that is not empty\n",
    "not_empty_name = f\"{DB_columns.NAME.value} IS NOT ''\"\n",
    "\n",
    "# Only rows with a name that is not \"Turret\"\n",
    "\n",
    "# Only rows with timestamp greater than 5\n",
    "timestamp_greater_than_5 = f\"{DB_columns.TIME.value} > 5\"\n",
    "\n",
    "# Only rows with pos_x and pos_y greater between [0, GAME_AREA_WIDTH]\n",
    "pos_x_greater_than_0 = f\"{DB_columns.POS_X.value} > 0\"\n",
    "pos_x_less_than_max = f\"{DB_columns.POS_X.value} < {GAME_AREA_WIDTH}\"\n",
    "pos_z_greater_than_0 = f\"{DB_columns.POS_Z.value} > 0\"\n",
    "pos_z_less_than_max = f\"{DB_columns.POS_Z.value} < {GAME_AREA_WIDTH}\"\n",
    "position_between_0_and_max = \" AND \".join(\n",
    "    [pos_x_greater_than_0, pos_x_less_than_max, pos_z_greater_than_0, pos_z_less_than_max])\n",
    "\n",
    "# Combine all above filters\n",
    "\n",
    "filter_conditions = \" AND \".join(\n",
    "    [not_empty_name, timestamp_greater_than_5, position_between_0_and_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cleaned indices to new table\n",
    "if RECREATE_CLEANED_DATA:\n",
    "    conn = sqlite3.connect(database_file)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\n",
    "        f\"INSERT INTO {cleaned_table_name} SELECT * FROM {tlol_db_table_name} WHERE {filter_conditions}\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check values from the new table\n",
    "\n",
    "conn = sqlite3.connect(database_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "page_size = 5\n",
    "\n",
    "rows = cursor.execute(\n",
    "    f\"SELECT * FROM {cleaned_table_name} LIMIT {page_size}\").fetchall()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indices for given columns\n",
    "\n",
    "conn = sqlite3.connect(database_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "def create_index(cursor, table_name, column_name):\n",
    "    if RECREATE_CLEANED_DATA:\n",
    "        cursor.execute(\n",
    "            f\"DROP INDEX IF EXISTS {table_name}_{column_name}_index\")\n",
    "        cursor.execute(\n",
    "            f\"CREATE INDEX IF NOT EXISTS {table_name}_{column_name}_index ON {table_name}({column_name})\")\n",
    "\n",
    "\n",
    "columns_to_be_indexed = [DB_columns.NAME.value]\n",
    "\n",
    "for column in columns_to_be_indexed:\n",
    "    create_index(cursor, cleaned_table_name, column)\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# List out indices\n",
    "\n",
    "conn = sqlite3.connect(database_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "print(create_compound_key_and_index(database_file, cleaned_table_name, [\n",
    "    DB_columns.GAME_ID.value, DB_columns.TEAM.value, DB_columns.NAME.value]))\n",
    "\n",
    "indices = cursor.execute(\n",
    "    f\"PRAGMA index_list({cleaned_table_name})\").fetchall()\n",
    "\n",
    "table_info = cursor.execute(\n",
    "    f\"PRAGMA table_info({cleaned_table_name})\").fetchall()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(indices)\n",
    "print(table_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(cursor, table_name, filter):\n",
    "    cursor.execute(\n",
    "        f\"SELECT COUNT(DISTINCT {DB_columns.COMPOUND_KEY.value}) FROM {table_name} WHERE {filter}\")\n",
    "    unique_keys = cursor.fetchone()[0]\n",
    "    cursor.execute(\n",
    "        f\"SELECT COUNT(*), {DB_columns.COMPOUND_KEY.value} FROM {table_name} WHERE {filter} GROUP BY {DB_columns.COMPOUND_KEY.value} ORDER BY {DB_columns.COMPOUND_KEY.value}\")\n",
    "    counts = cursor.fetchall()\n",
    "    return unique_keys, counts\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(database_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "filter = \"1=1\"  # f\"{DB_columns.NAME.value} = 'Ezreal'\"\n",
    "\n",
    "unique_keys, counts = get_counts(cursor, cleaned_table_name, filter)\n",
    "conn.close()\n",
    "\n",
    "offsets = []\n",
    "cumulative_sum = 0\n",
    "\n",
    "for count in counts:\n",
    "    offsets.append(cumulative_sum)\n",
    "    cumulative_sum += count[0]\n",
    "offsets.append(cumulative_sum)\n",
    "\n",
    "unique_keys, cumulative_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to fetch from the database\n",
    "\n",
    "data_features = [DB_columns.NORMALIZED_POS_X.value,\n",
    "                 DB_columns.NORMALIZED_POS_Z.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_by_compound_key(cursor, table_name, offset, limit, filter):\n",
    "    return cursor.execute(\n",
    "        f\"SELECT {','.join(data_features)} FROM {table_name} WHERE {filter} ORDER BY {DB_columns.COMPOUND_KEY.value} LIMIT {limit} OFFSET {offset}\"\n",
    "    ).fetchall()\n",
    "\n",
    "\n",
    "def get_data(cursor, table_name, offset, limit, counts, offsets, total, filter):\n",
    "    all_rows = get_data_by_compound_key(\n",
    "        cursor, table_name, offset, limit, filter)\n",
    "\n",
    "    rows_per_key = defaultdict(list)\n",
    "\n",
    "    for i in tqdm(range(total)):\n",
    "        result_offset = offsets[i] - offset\n",
    "        result_count = counts[i][0]\n",
    "        key_slice = all_rows[result_offset:result_offset + result_count]\n",
    "        if len(key_slice) > 0:\n",
    "            rows_per_key[counts[i][1]].extend(key_slice)\n",
    "\n",
    "    return rows_per_key\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(database_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Adjust this based on the actual number of compound keys\n",
    "total = int(unique_keys / 30)\n",
    "total_count = offsets[total]\n",
    "\n",
    "batches = min(total, 10)  # Update the number of batches as required\n",
    "\n",
    "batch_key_counts = [int(total / batches)] * batches\n",
    "batch_key_counts[-1] += total % batches\n",
    "\n",
    "batch_offsets = []\n",
    "batch_counts = []\n",
    "batch_cumulative_sum = []\n",
    "cumulative_sum = 0\n",
    "\n",
    "for count in batch_key_counts:\n",
    "    batch_offsets.append(offsets[cumulative_sum:count + cumulative_sum])\n",
    "    batch_counts.append(counts[cumulative_sum:count + cumulative_sum])\n",
    "    batch_cumulative_sum.append(cumulative_sum)\n",
    "    cumulative_sum += count\n",
    "\n",
    "rows_per_key = defaultdict(list)\n",
    "with tqdm(total=total_count) as pbar:  # Adjust the progress bar total if needed\n",
    "    for i in range(batches):\n",
    "        offset = offsets[batch_cumulative_sum[i]]\n",
    "        limit = offsets[batch_cumulative_sum[i] + batch_key_counts[i]] - offset\n",
    "        keys_per_batch = batch_key_counts[i]\n",
    "\n",
    "        offsets_of_batch = batch_offsets[i]\n",
    "        counts_of_batch = batch_counts[i]\n",
    "\n",
    "        batch_rows_per_key = get_data(\n",
    "            cursor, cleaned_table_name, offset, limit, counts_of_batch, offsets_of_batch, keys_per_batch, filter)\n",
    "        for key, rows in batch_rows_per_key.items():\n",
    "            pbar.update(len(rows))\n",
    "            rows_per_key[key].extend(rows)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"Got {len(rows_per_key)} keys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(list(rows_per_key.values()), dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(rows, H, T):\n",
    "    num_sequences = len(rows) - H - T + 1\n",
    "    if num_sequences <= 0:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    X = np.lib.stride_tricks.sliding_window_view(\n",
    "        rows, window_shape=(H, rows.shape[1]))[:-T]\n",
    "    y = rows[H+T-1:num_sequences+H+T-1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_sequences_from_database_rows(data, H, T):\n",
    "    batch_size = 1000\n",
    "    X_list, y_list = [], []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        for rows in batch:\n",
    "            if len(rows) < H + T:\n",
    "                continue\n",
    "            # # Ensure all sequences are from the same points in time\n",
    "            # equilength_rows = np.array(rows)[max_H-H:-(max_T-T+1)]\n",
    "            _X, _y = create_sequences(np.array(rows), H, T)\n",
    "            if _X.size > 0 and _y.size > 0:\n",
    "                X_list.append(_X)\n",
    "                y_list.append(_y)\n",
    "\n",
    "    if X_list and y_list:\n",
    "        X = np.concatenate(X_list, axis=0)\n",
    "        y = np.concatenate(y_list, axis=0)\n",
    "    else:\n",
    "        X, y = np.array([]), np.array([])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "H = 3\n",
    "T = 1\n",
    "\n",
    "X, y = create_sequences_from_database_rows(data, H, T)\n",
    "\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics about the data\n",
    "\n",
    "# How many row groups are there?\n",
    "print(f\"Number of rows: {data.shape[0]}\")\n",
    "\n",
    "# How many rows in a group on average?\n",
    "row_counts = [len(row) for row in data]\n",
    "print(f\"Average number of rows in a group: {np.mean(row_counts)}\")\n",
    "\n",
    "# Statistics about the sequences\n",
    "\n",
    "# How many sequences are there?\n",
    "print(f\"Number of sequences: {X.shape[0]}\")\n",
    "\n",
    "# Plot the rows group sizes\n",
    "\n",
    "plt.hist(row_counts, bins=20)\n",
    "plt.xlabel('Row group size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Row group size distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y):\n",
    "    indices = np.arange(len(X))\n",
    "    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "        X, y, indices, test_size=0.2, random_state=42, shuffle=False)\n",
    "    # Flatten the input sequences\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    y_train = y_train.reshape(y_train.shape[0], -1)\n",
    "    y_test = y_test.reshape(y_test.shape[0], -1)\n",
    "    return X_train, X_test, y_train, y_test, idx_train, idx_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = split_data(X, y)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 2\n",
    "T = 3\n",
    "\n",
    "X, y = create_sequences_from_database_rows(data, H, T)\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = split_data(X, y)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return mse, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that predicts the next position by taking a random step in the direction of the average velocity\n",
    "class RandomStepModel:\n",
    "    def __init__(self, H, T):\n",
    "        self.H = H\n",
    "        self.T = T\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Assuming X_test shape is (num_samples, H*2)\n",
    "        num_samples = X_test.shape[0]\n",
    "\n",
    "        # Reshape X_test to (num_samples, H, 2)\n",
    "        X_test_reshaped = X_test.reshape(num_samples, self.H, 2)\n",
    "\n",
    "        # Get average velocity for each sequence\n",
    "        avg_velocity = (\n",
    "            X_test_reshaped[:, -1, :] - X_test_reshaped[:, 0, :]) / self.H\n",
    "\n",
    "        # Calculate the length of the step for timestep T\n",
    "        step = avg_velocity * self.T\n",
    "\n",
    "        # Calculate a random direction for each sample\n",
    "        direction = np.random.uniform(-1, 1, size=step.shape)\n",
    "\n",
    "        # Normalize the direction to ensure the step size is consistent\n",
    "        norms = np.linalg.norm(direction, axis=1, keepdims=True)\n",
    "        norms[norms == 0] = 1  # Avoid division by zero\n",
    "        direction /= norms\n",
    "\n",
    "        # Calculate the random step\n",
    "        random_step = step * direction\n",
    "\n",
    "        # Get the last position in the sequence for each sample\n",
    "        last_position = X_test_reshaped[:, -1, :]\n",
    "\n",
    "        # Calculate the prediction\n",
    "        y_pred = last_position + random_step\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "random_step_model = RandomStepModel(H, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_random_walk, rmse_random_walk, r2_random_walk = calculate_metrics(\n",
    "    random_step_model, X_test, y_test)\n",
    "\n",
    "\n",
    "print(f'Mean Squared Error (Random Walk): {mse_random_walk}')\n",
    "print(f'Root Mean Squared Error (Random Walk): {rmse_random_walk}')\n",
    "print(f'R-squared (Random Walk): {r2_random_walk}')\n",
    "\n",
    "# Visualize the random walk predictions\n",
    "# create_prediction_animation(X_test, y_pred_random_walk, y_test, map_image_path, zoom_range, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that selects the last position as the prediction\n",
    "\n",
    "class LastPositionModel:\n",
    "    def __init__(self, H):\n",
    "        self.H = H\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Unflatten X_test\n",
    "        X_test_reshaped = X_test.reshape(X_test.shape[0], self.H, 2)\n",
    "        # Get the last position for each sequence\n",
    "        return X_test_reshaped[:, -1, :]\n",
    "\n",
    "\n",
    "# Create the model\n",
    "last_position_model = LastPositionModel(H)\n",
    "\n",
    "mse_last_position, rmse_last_position, r2_last_position = calculate_metrics(\n",
    "    last_position_model, X_test, y_test)\n",
    "\n",
    "print(f'Mean Squared Error (Last Position): {mse_last_position}')\n",
    "print(f'Root Mean Squared Error (Last Position): {rmse_last_position}')\n",
    "print(f'R-squared (Last Position): {r2_last_position}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that selects a random position from a given range as the prediction\n",
    "\n",
    "class RandomPositionModel:\n",
    "    def __init__(self, H, range_x, range_y):\n",
    "        self.H = H\n",
    "        self.range_x = range_x\n",
    "        self.range_y = range_y\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Generate random positions for each sequence\n",
    "        x = np.random.uniform(\n",
    "            self.range_x[0], self.range_x[1], size=X_test.shape[0])\n",
    "        y = np.random.uniform(\n",
    "            self.range_y[0], self.range_y[1], size=X_test.shape[0])\n",
    "        return np.array(list(zip(x, y)))\n",
    "\n",
    "\n",
    "range_x = (0, 1)\n",
    "range_y = (0, 1)\n",
    "\n",
    "random_position_model = RandomPositionModel(H, range_x, range_y)\n",
    "\n",
    "mse_random_position, rmse_random_position, r2_random_position = calculate_metrics(\n",
    "    random_position_model, X_test, y_test)\n",
    "\n",
    "print(f'Mean Squared Error (Random Position): {mse_random_position}')\n",
    "print(f'Root Mean Squared Error (Random Position): {rmse_random_position}')\n",
    "print(f'R-squared (Random Position): {r2_random_position}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousMotionModel:\n",
    "    def __init__(self, H, T):\n",
    "        self.H = H\n",
    "        self.T = T\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Assuming X_test shape is (num_samples, H*2)\n",
    "        num_samples = X_test.shape[0]\n",
    "\n",
    "        # Reshape X_test to (num_samples, H, 2)\n",
    "        X_test_reshaped = X_test.reshape(num_samples, self.H, 2)\n",
    "\n",
    "        # Create weights that increase linearly\n",
    "        weights = np.linspace(1, 2, self.H)  # Example: weights from 1 to 2\n",
    "        weights /= weights.sum()  # Normalize weights to sum to 1\n",
    "\n",
    "        # Get weighted average velocity for each sequence\n",
    "        weighted_positions = X_test_reshaped * weights[:, np.newaxis]\n",
    "        weighted_avg_velocity = (\n",
    "            weighted_positions.sum(axis=1) - X_test_reshaped[:, 0, :]\n",
    "        ) / self.H\n",
    "\n",
    "        # Calculate the length of the step for timestep T\n",
    "        step = weighted_avg_velocity * self.T\n",
    "\n",
    "        # Get the last position in the sequence for each sample\n",
    "        last_position = X_test_reshaped[:, -1, :]\n",
    "\n",
    "        # Calculate the prediction\n",
    "        y_pred = last_position + step\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_motion_model = ContinuousMotionModel(H, T)\n",
    "\n",
    "mse_continuous_motion, rmse_continuous_motion, r2_continuous_motion = calculate_metrics(\n",
    "    continuous_motion_model, X_test, y_test)\n",
    "\n",
    "print(f'Mean Squared Error (Continuous Motion): {mse_continuous_motion}')\n",
    "print(f'Root Mean Squared Error (Continuous Motion): {rmse_continuous_motion}')\n",
    "print(f'R-squared (Continuous Motion): {r2_continuous_motion}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(rows, H, T):\n",
    "    num_sequences = len(rows) - H - T + 1\n",
    "    if num_sequences <= 0:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    X = np.lib.stride_tricks.sliding_window_view(\n",
    "        rows, window_shape=(H, rows.shape[1]))[:-T]\n",
    "    X = X.reshape(X.shape[0], H, -1)\n",
    "    y = rows[H+T-1:num_sequences+H+T-1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_sequences_from_database_rows(data, H, T, max_H=None, max_T=None, batch_size=1000):\n",
    "    max_H = max_H or H\n",
    "    max_T = max_T or T\n",
    "    X_list, y_list = [], []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        for rows in batch:\n",
    "            if len(rows) < H + T:\n",
    "                continue\n",
    "            # Ensure all sequences are from the same points in time\n",
    "            equilength_rows = np.array(rows)[max_H-H:-(max_T-T+1)]\n",
    "            _X, _y = create_sequences(equilength_rows, H, T)\n",
    "            if _X.size > 0 and _y.size > 0:\n",
    "                X_list.append(_X)\n",
    "                y_list.append(_y)\n",
    "\n",
    "    if X_list and y_list:\n",
    "        X = np.concatenate(X_list, axis=0)\n",
    "        y = np.concatenate(y_list, axis=0)\n",
    "    else:\n",
    "        X, y = np.array([]), np.array([])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def calculate_sequences_in_batches(H_values, T_values, data, batch_size=1000, split=True, test_size=0.2, random_state=None, show_progress=True):\n",
    "    sequences = {}\n",
    "\n",
    "    # Get the max H and T values\n",
    "    max_H = max(H_values)\n",
    "    max_T = max(T_values)\n",
    "\n",
    "    for H in tqdm(H_values, desc='H loop', leave=False):\n",
    "        for T in tqdm(T_values, desc='T loop', leave=False):\n",
    "            X, y = create_sequences_from_database_rows(\n",
    "                data, H, T, max_H, max_T, batch_size)\n",
    "            if X.size > 0 and y.size > 0:\n",
    "                if split:\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                        X, y, test_size=test_size, random_state=random_state)\n",
    "                    sequences[(H, T)] = (X_train, X_test, y_train, y_test)\n",
    "                else:\n",
    "                    sequences[(H, T)] = (X, None, y, None)\n",
    "            else:\n",
    "                sequences[(H, T)] = (np.array([]), np.array([]),\n",
    "                                     np.array([]), np.array([]))\n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = calculate_sequences_in_batches(\n",
    "    [2, 3], [1, 2], data, batch_size=1000, split=False, test_size=0.2, random_state=42)\n",
    "\n",
    "last_positions = [sequence[0][:, -1, :] for sequence in sequences.values()]\n",
    "\n",
    "next_positions = [sequence[2] for sequence in sequences.values()]\n",
    "\n",
    "print(np.sum(np.subtract(last_positions, next_positions), axis=1))\n",
    "print([key for key, value in sequences.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all four models\n",
    "\n",
    "def compare_models(H_values, T_values, data, model_getters, train=True):\n",
    "    absolute_errors = defaultdict(list)\n",
    "    rmse_results = defaultdict(int)\n",
    "    trained_models = {}\n",
    "    max_H = max(H_values)\n",
    "    max_T = max(T_values)\n",
    "    pbar = tqdm(total=len(H_values) * len(T_values) *\n",
    "                len(model_getters), desc='Model loop')\n",
    "    for H in H_values:\n",
    "        for T in tqdm(T_values, desc=f'H={H}', leave=False):\n",
    "            # Calculate the sequence on the fly\n",
    "            sequence = create_sequences_from_database_rows(\n",
    "                data, H, T, max_H, max_T)\n",
    "            X_train, y_train = sequence\n",
    "            if X_train.size == 0 or y_train.size == 0:\n",
    "                pbar.update(len(model_getters))\n",
    "                continue\n",
    "            models = {model_name: (model_getter(\n",
    "                H, T), features) for model_name, (model_getter, features) in model_getters.items()}\n",
    "            for model_name, (model, features) in tqdm(models.items(), desc=f'T={T}', leave=False):\n",
    "                X_train_features = X_train[:, :, [\n",
    "                    data_features.index(feature) for feature in features]]\n",
    "                X_train_reshaped = X_train.reshape(\n",
    "                    X_train_features.shape[0], -1)\n",
    "                X_train_coords = X_train_reshaped  # [:, :, :2].reshape(\n",
    "                # X_train.shape[0], -1)\n",
    "                model.fit(X_train_coords, y_train)\n",
    "                y_pred = model.predict(X_train_coords)\n",
    "                rmse = root_mean_squared_error(y_train, y_pred)\n",
    "                rmse_results[(H, T, model_name)] += rmse\n",
    "                absolute_errors[(H, T, model_name)] = np.abs(y_train - y_pred)\n",
    "                if train:\n",
    "                    trained_models[(H, T, model_name)] = model\n",
    "                pbar.update(1)\n",
    "    pbar.close()\n",
    "    return trained_models, rmse_results, absolute_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H values on a logarithmic scale\n",
    "H_values = [100, 200, 300]\n",
    "T_values = [50, 100, 150]\n",
    "\n",
    "# sequence_data = calculate_sequences_in_batches(H_values, T_values, data)\n",
    "\n",
    "# print(f\"Calculated sequences for {len(sequence_data)} combinations of H and T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_getters = {\n",
    "    'Linear Model': (lambda H, T: LinearRegression(), [DB_columns.NORMALIZED_POS_X.value, DB_columns.NORMALIZED_POS_Z.value]),\n",
    "    'Random Step Model': (lambda H, T: RandomStepModel(H, T), [DB_columns.NORMALIZED_POS_X.value, DB_columns.NORMALIZED_POS_Z.value]),\n",
    "    'Last Position Model': (lambda H, T: LastPositionModel(H), [DB_columns.NORMALIZED_POS_X.value, DB_columns.NORMALIZED_POS_Z.value]),\n",
    "    'Random Position Model': (lambda H, T: RandomPositionModel(H, normalized_zoom_range[0], normalized_zoom_range[1]), [DB_columns.NORMALIZED_POS_X.value, DB_columns.NORMALIZED_POS_Z.value]),\n",
    "    'Continuous Motion Model': (lambda H, T: ContinuousMotionModel(H, T), [DB_columns.NORMALIZED_POS_X.value, DB_columns.NORMALIZED_POS_Z.value])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models, rmse_results, absolute_errors = compare_models(\n",
    "    H_values, T_values, data, model_getters)\n",
    "\n",
    "results_table = pd.DataFrame(rmse_results, index=['Root Mean Squared Error']).T\n",
    "results_table.index.names = ['H', 'T', 'Model']\n",
    "results_table = results_table.sort_index()\n",
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PLOT = True\n",
    "\n",
    "# Plot the results table, one table per model\n",
    "subplot_amount = len(model_getters)\n",
    "# If just a single value combination per model, plot them all in a bar chart\n",
    "if len(H_values) == 1 and len(T_values) == 1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    results_table.unstack().plot(ax=ax, kind='bar', logy=LOG_PLOT)\n",
    "    ax.set_xlabel('H')\n",
    "    ax.set_ylabel('RMSE')\n",
    "    ax.legend(title='T')\n",
    "else:\n",
    "    # If just one T value, plot all models in the same plot\n",
    "    if len(T_values) == 1:\n",
    "        # The X-axis is currently tuple, take the first value only to have a single value\n",
    "        results_table.xs(T_values[0], level='T').unstack().plot(\n",
    "            kind='line', logy=LOG_PLOT)\n",
    "\n",
    "        plt.xlabel('H')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend(title='T')\n",
    "    elif len(H_values) == 1:\n",
    "        # If just one H value, plot all models in the same plot\n",
    "        results_table.xs(H_values[0], level='H').unstack().plot(\n",
    "            kind='line', logy=LOG_PLOT)\n",
    "\n",
    "        plt.xlabel('T')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend(title=f'H={H_values[0]}')\n",
    "    else:\n",
    "        fig, axs = plt.subplots(\n",
    "            subplot_amount, 1, figsize=(10, 10*subplot_amount))\n",
    "        for i, model_name in enumerate(model_getters.keys()):\n",
    "            # Plot the results for the model, H on the x-axis, RMSE on the y-axis, and different lines for different T\n",
    "            results_table.xs(model_name, level='Model').unstack().plot(\n",
    "                ax=axs[i] if subplot_amount > 1 else axs, title=model_name, logy=LOG_PLOT)\n",
    "            (axs[i] if subplot_amount > 1 else axs).set_xlabel('H')\n",
    "            (axs[i] if subplot_amount > 1 else axs).set_ylabel('RMSE')\n",
    "            (axs[i] if subplot_amount > 1 else axs).legend(title='T')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of absolute errors for each model\n",
    "if len(H_values) == 1 and len(T_values) == 1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    for (H, T, name), errors in absolute_errors.items():\n",
    "        errors = errors.flatten()\n",
    "        ax.hist(errors, bins=100, alpha=0.5,\n",
    "                label=f'H={H}, T={T}', range=(0, 1), log=LOG_PLOT)\n",
    "    ax.set_xlabel('Absolute Error')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "elif len(T_values) == 1:\n",
    "    fig, axs = plt.subplots(\n",
    "        len(H_values), 1, figsize=(10, 10*len(H_values)))\n",
    "    for i, model_name in enumerate(model_getters.keys()):\n",
    "        for (H, T, name), errors in [item for item in absolute_errors.items() if item[0][2] == model_name]:\n",
    "            errors = errors.flatten()\n",
    "            axs[i].hist(errors, bins=100, alpha=0.5,\n",
    "                        label=f'{name} H={H}', range=(0, 1), log=LOG_PLOT)\n",
    "        axs[i].set_title(f'{model_name}')\n",
    "        axs[i].set_xlabel('Absolute Error')\n",
    "        axs[i].set_ylabel('Frequency')\n",
    "        axs[i].legend()\n",
    "elif len(H_values) == 1:\n",
    "    fig, axs = plt.subplots(\n",
    "        len(T_values), 1, figsize=(10, 10*len(T_values)))\n",
    "    for i, model_name in enumerate(model_getters.keys()):\n",
    "        for (H, T, name), errors in [item for item in absolute_errors.items() if item[0][2] == model_name]:\n",
    "            errors = errors.flatten()\n",
    "            axs[i].hist(errors, bins=100, alpha=0.5,\n",
    "                        label=f'{name} T={T}', range=(0, 1), log=LOG_PLOT)\n",
    "        axs[i].set_title(f'{model_name}')\n",
    "        axs[i].set_xlabel('Absolute Error')\n",
    "        axs[i].set_ylabel('Frequency')\n",
    "        axs[i].legend()\n",
    "else:\n",
    "    for i, model_name in enumerate(model_getters.keys()):\n",
    "        # Determine the number of rows and columns for the subplots\n",
    "        num_rows = len(H_values)\n",
    "        num_cols = len(T_values)\n",
    "\n",
    "        # Create a figure and a grid of subplots\n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize=(\n",
    "            15, 10), constrained_layout=True)\n",
    "\n",
    "        for i, H in enumerate(H_values):\n",
    "            for j, T in enumerate(T_values):\n",
    "                ax = axs[i, j]\n",
    "                errors = absolute_errors[(H, T, model_name)]\n",
    "                ax.hist(errors, bins=20, alpha=0.8, range=(0, 1), log=LOG_PLOT)\n",
    "                ax.set_title(f'H={H}, T={T}')\n",
    "                ax.set_xlabel('Absolute Error')\n",
    "                ax.set_ylabel('Frequency')\n",
    "\n",
    "        plt.suptitle('Distribution of Absolute Errors for ' + model_name)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models to disk\n",
    "\n",
    "for (H, T, model_name), model in trained_models.items():\n",
    "    joblib.dump(\n",
    "        model, f'models/{model_name.replace(\" \", \"_\")}_H_{H}_T_{T}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models from disk - get models and load them into the trained_models dictionary\n",
    "\n",
    "# Get all H and T values from the file names\n",
    "import os\n",
    "import re\n",
    "\n",
    "loaded_models = {}\n",
    "\n",
    "model_files = os.listdir('models')\n",
    "model_files = [file for file in model_files if file.endswith('.joblib')]\n",
    "pattern = re.compile(r'(\\w+)_(\\d+)_(\\d+).joblib')\n",
    "for file in model_files:\n",
    "    match = pattern.match(file)\n",
    "    if match:\n",
    "        model_name, H, T = match.groups()\n",
    "        H = int(H)\n",
    "        T = int(T)\n",
    "        model = joblib.load(f'models/{file}')\n",
    "        loaded_models[(H, T, model_name)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small example prediction animation for each model\n",
    "# example_data = data[:5]\n",
    "\n",
    "# trained_models = {key: trained_models[key]\n",
    "#                   for key in list(trained_models.keys())[:4]}\n",
    "# create_prediction_animation_for_models(\n",
    "#     trained_models, example_data, map_image_path, zoom_range, options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
